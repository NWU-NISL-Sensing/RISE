{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e606f7",
   "metadata": {},
   "source": [
    "# Robust Wireless Sensing using Probabilistic and Statistical Assessments ：Artifact - Interactive Demo\n",
    "<img src=\"./overview2.jpg\" width=\"100%\" style=\"float:left\" />\n",
    "\n",
    "\n",
    "\n",
    "## Step 1. Training the underlying model and use it to predict test samples.\n",
    "\n",
    "## Step 2. Training an anomaly detector for each class according to the training samples.\n",
    "## Step 3. Evaluate the ability of the underlying model to predict the test sample.\n",
    "## Step 4. Performance of the RISE.\n",
    "## Step 5. Performance of the underlying model after RISE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd450db7",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "This interactive Jupyter notebook provides an example to show the performance of RISE. \n",
    "## Instructions for Experimental Workflow:\n",
    "Select each cell in turn and use “Cell” > “Run Cell” from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure all previous cells have been executed.\n",
    "## Important Notes\n",
    "#### Some cells can take a few minutes to complete; please wait for the results until step to the next cell.\n",
    "\n",
    "## Step 1. Performance of the underlying model without RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762ae2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------  The performance of the underlying model without RISE --------\n",
      "\n",
      "The accuracy without RISE:  0.7833655705996132\n",
      "Confusion matrix without RISE: \n",
      " [[  0   0   0   1 103]\n",
      " [  0 102   0   0   1]\n",
      " [  0   0 103   0   0]\n",
      " [  0   0   1 103   0]\n",
      " [  0   0   6   0  97]]\n"
     ]
    }
   ],
   "source": [
    "#Import different data sets\n",
    "\n",
    "#### You can change S1  S2  S3  S4  S5\n",
    "# from WiG_test.S1.test_start import start\n",
    "# dict1 = {}\n",
    "# dict1['class_index'] = 0\n",
    "# dict1['class_num'] = 6\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif = start(dict1,'fe_wig','label')\n",
    "\n",
    "#### You can change S1  S2  S3  S4  S5\n",
    "# from WiAG_test.S1.test_start import start\n",
    "# dict1 = {}\n",
    "# dict1['class_index'] = 3\n",
    "# dict1['class_num'] = 6\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif = start(dict1,'fe_dwt_mobisys9','label') \n",
    "\n",
    "# #### You can change L1 L2  L3  L4  L5\n",
    "# from WiAG_C_test.L1.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()  \n",
    "\n",
    "# #### You can change L1 L2  L3  L4  L5\n",
    "# from WiAG_O_test.L1.test_start import start\n",
    "# dict1 = {}\n",
    "# dict1['class_index'] = 3\n",
    "# dict1['class_num'] = 6\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif = start(dict1,'fe_dwt_mobisys9','label') \n",
    "\n",
    "# #### You can change S1 S2 S3 S4 S5\n",
    "# from TACT_test.S1.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()  \n",
    "\n",
    "# #### You can change S1 S2 S3 S4 S5\n",
    "# from AllSee_test.S1.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start() \n",
    "\n",
    "# #### You can change S1 S2 S3 S4 S5\n",
    "# from EI_test.S1.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start() \n",
    "\n",
    "# #### You can change S1 S2 S3 S4 S5\n",
    "# from WiWho_test.S1.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start() \n",
    "\n",
    "# #### You can change S1 S2 S3 S4 S5\n",
    "# from WifiU_test.S5.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "# #### You can change N _1 _2 _3 _4\n",
    "# from VibWrite_R_test.N.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "# #### You can change N _1 _2 _3 _4\n",
    "# from VibWrite_A_test.N.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "# #### You can change S D M W \n",
    "# from Taprint_R_test.W.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "# #### You can change S D M W \n",
    "# from Taprint_A_test.W.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "# #### You can change S DP T M\n",
    "# from M_Touch.S.test_start import start\n",
    "# x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "\n",
    "#### You can change RP LP DC DB \n",
    "from UDO_Free.DC.test_start import start\n",
    "x_train1, y_train1, x_test1, y_test1, myclassifier, y_true_dif, y_pred_dif,class_num,class_index = start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c419d08",
   "metadata": {},
   "source": [
    "## Step 2. Training an Anomaly Detector for each class according to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbda47f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------  Training one-class SVM model for each class -----------------\n",
      "\n",
      "clf_p_prob_0\n",
      "\n",
      "clf_p_prob_1\n",
      "\n",
      "clf_p_prob_2\n",
      "\n",
      "clf_p_prob_3\n",
      "\n",
      "clf_p_prob_4\n",
      "\n",
      "---------------------- The number of classes is 5 ----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################ Trainning Anomaly Detector  ###############################  \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nonconformist.nc import MarginErrFunc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Numerical issues were encountered \")\n",
    "import sys\n",
    "sys.path.insert(0,'/root/RISE-Version2/')\n",
    "from Statistical_vector.statistical_vector import train_statistical_vector, test_statistical_vector_param, non_condition_p\n",
    "import random\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "import sklearn.ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "## Probability vector and statistical vector are calculated according to the training data\n",
    "skf = StratifiedKFold(n_splits=3, random_state=0,shuffle=True)\n",
    "cal_proba = np.empty(shape=[0, (class_num)*4])\n",
    "cal_score = np.empty(shape=[0, 1*4])\n",
    "train_proba = np.empty(shape=[0, (class_num)*4])\n",
    "train_nonconformity = np.empty(shape=[0, (class_num)*4])\n",
    "train_p = np.empty(shape=[0, (class_num)*4])\n",
    "cal_label = np.empty(shape=[0,1])\n",
    "train_label = np.empty(shape=[0,1])\n",
    "for train_index, cal_index in skf.split(x_train1, y_train1):\n",
    "    data_train = x_train1[train_index, :] \n",
    "    label_train = y_train1[train_index]\n",
    "    data_cal = x_train1[cal_index, :]\n",
    "    label_cal = y_train1[cal_index]\n",
    "    \n",
    "    train_p1_svm_, train_proba1_svm_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[0], non_Func = MarginErrFunc(), significance=None) \n",
    "    train_p1_rf_, train_proba1_rf_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[1], non_Func = MarginErrFunc(), significance=None) \n",
    "    train_p1_lr_, train_proba1_lr_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[4], non_Func = MarginErrFunc(), significance=None)\n",
    "    train_p1_gbc_, train_proba1_gbc_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[5], non_Func = MarginErrFunc(), significance=None)\n",
    "    \n",
    "    train_proba1 = np.hstack((train_proba1_svm_, train_proba1_rf_, train_proba1_lr_, train_proba1_gbc_))\n",
    "    train_p1 = np.hstack((train_p1_svm_, train_p1_rf_, train_p1_lr_, train_p1_gbc_))\n",
    "    train_proba = np.append(train_proba, train_proba1, axis=0)  \n",
    "    train_p = np.append(train_p, train_p1, axis=0)  \n",
    "    train_label = np.append(train_label,y_train1[train_index]) \n",
    "    \n",
    "p_thr = 0.1\n",
    "rows_ = []  \n",
    "for row_ in range(train_p.shape[0]):\n",
    "    if (max(train_p[row_,0:0+class_num]) > p_thr) & (max(train_p[row_,class_num:class_num*1+class_num]) > p_thr) \\\n",
    "        & (max(train_p[row_,class_num*2:class_num*2+class_num]) > p_thr) & (max(train_p[row_,class_num*3:class_num*3+class_num]) > p_thr) :               \n",
    "            rows_.append(row_)\n",
    "        \n",
    "pro_n_p = np.hstack((train_proba, train_p))\n",
    "group_it = pro_n_p[rows_,:] \n",
    "train_label1 = train_label[rows_] \n",
    "\n",
    "group_it = preprocessing.scale(group_it)\n",
    "   \n",
    "index = [t for t in range(group_it.shape[0])] \n",
    "random.shuffle(index)\n",
    "group_it1 = group_it[index] \n",
    "train_p_lable1 = train_label1[index]\n",
    "\n",
    "\n",
    "\n",
    "####Training an anomaly detector for each class\n",
    "\n",
    "##Generalize probability vectors and statistical vectors for each class\n",
    "names = locals()\n",
    "fe_p_prob = np.hstack((train_p_lable1.reshape(-1,1),group_it1))\n",
    "for tt in range(class_num):\n",
    "    names['fe_p_prob_%s'%tt] = fe_p_prob[fe_p_prob[:,0]==tt]\n",
    "    names['fe_p_prob_%s'%tt] = np.delete(names['fe_p_prob_%s'%tt], 0, 1)  \n",
    "\n",
    "print('\\n---------------  Training one-class SVM model for each class -----------------\\n')    \n",
    "##training one-class SVM model for each class\n",
    "for tt in range(class_num):\n",
    "    names['./save_model/clf_p_prob_%s'%tt] = svm.OneClassSVM(nu=0.5,kernel=\"linear\" )\n",
    "    names['./save_model/clf_p_prob_%s'%tt].fit(names['fe_p_prob_%s'%tt])\n",
    "    print('clf_p_prob_'+ str(tt)+'\\n')\n",
    "    ##save one-class SVM model for each class\n",
    "    joblib.dump(names['./save_model/clf_p_prob_%s'%tt],'./save_model/clf_p_prob_%s'%tt+'.model')\n",
    "    \n",
    "\n",
    "##Ensemble Learning\n",
    "clf_dif1 = sklearn.ensemble.RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "clf_dif2 = svm.SVC(probability = True, random_state=0)\n",
    "clf_dif3 = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "clf_dif4 = LogisticRegression(random_state=0)\n",
    "clf_dif7 = AdaBoostClassifier(random_state=0)   \n",
    "clf_pit = VotingClassifier(estimators = [('rf',clf_dif1),('svm',clf_dif2),\n",
    "('knn',clf_dif3),('lr',clf_dif4),('AdaBoost',clf_dif7)],voting='hard')      \n",
    "clf_pit.fit(group_it1,train_p_lable1)\n",
    "joblib.dump(clf_pit,'./save_model/clf_pit.model') \n",
    "\n",
    "print('---------------------- The number of classes is ' + str(class_num) + ' ----------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab9215",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the ability of the underlying model to predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b883e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/root/miniconda3/envs/rise/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------  Evaluate the ability of the underlying model to predict the test data -----------------\n",
      "\n",
      "[1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#####Calculate the probability vector and statistical vector of the test set\n",
    "           \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "calibration_portion = 0.5\n",
    "split = StratifiedShuffleSplit(n_splits=1,\n",
    "                   test_size=calibration_portion)\n",
    "for train, cal in split.split(x_train1,y_train1):\n",
    "    cal_scores1_svm = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_rf = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_lr = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_gbc = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    test_svmnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_rfnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_lrnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_gbcnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_svm1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_rf1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_lr1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_gbc1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    for repeat in range(10):\n",
    "        train_sample = np.random.choice(train.size, train.size, replace=True)\n",
    "        data_train = x_train1[train_sample, :]\n",
    "        label_train = y_train1[train_sample]\n",
    "        data_cal = x_train1[cal, :]\n",
    "        label_cal = y_train1[cal]\n",
    "        data_test = x_test1\n",
    "\n",
    "        cal_scores_svm, test_svmnc_score, test_svm_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[0], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_svm = np.hstack((cal_scores1_svm,cal_scores_svm.reshape(-1,1)))\n",
    "        test_svmnc1_score = np.dstack((test_svmnc1_score,test_svmnc_score))\n",
    "        test_svm1_proba = np.dstack((test_svm1_proba,test_svm_proba))\n",
    "  \n",
    "        cal_scores_rf, test_rfnc_score, test_rf_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[1], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_rf = np.hstack((cal_scores1_rf,cal_scores_rf.reshape(-1,1)))\n",
    "        test_rfnc1_score = np.dstack((test_rfnc1_score,test_rfnc_score))\n",
    "        test_rf1_proba = np.dstack((test_rf1_proba,test_rf_proba))\n",
    "          \n",
    "        cal_scores_lr, test_lrnc_score, test_lr_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[4], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_lr = np.hstack((cal_scores1_lr,cal_scores_lr.reshape(-1,1)))\n",
    "        test_lrnc1_score = np.dstack((test_lrnc1_score,test_lrnc_score))\n",
    "        test_lr1_proba = np.dstack((test_lr1_proba,test_lr_proba))\n",
    "        \n",
    "        cal_scores_gbc, test_gbcnc_score, test_gbc_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[5], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_gbc = np.hstack((cal_scores1_gbc,cal_scores_gbc.reshape(-1,1)))\n",
    "        test_gbcnc1_score = np.dstack((test_gbcnc1_score,test_gbcnc_score))\n",
    "        test_gbc1_proba = np.dstack((test_gbc1_proba,test_gbc_proba))\n",
    "        \n",
    "        \n",
    "    ##Nonconformity score of the validation set of the test set    \n",
    "    cal_scores2_svm = np.mean(np.delete(cal_scores1_svm,0,1), axis=1)\n",
    "    cal_scores2_rf = np.mean(np.delete(cal_scores1_rf,0,1), axis=1)\n",
    "    cal_scores2_lr = np.mean(np.delete(cal_scores1_lr,0,1), axis=1)\n",
    "    cal_scores2_gbc = np.mean(np.delete(cal_scores1_gbc,0,1), axis=1) \n",
    "    \n",
    "    ##Nonconformity score of the test set\n",
    "    test_svmnc2_score = np.mean(np.delete(test_svmnc1_score,0,2), axis=2)\n",
    "    test_rfnc2_score = np.mean(np.delete(test_rfnc1_score,0,2), axis=2)\n",
    "    test_lrnc2_score = np.mean(np.delete(test_lrnc1_score,0,2), axis=2)\n",
    "    test_gbcnc2_score = np.mean(np.delete(test_gbcnc1_score,0,2), axis=2)\n",
    "    \n",
    "    ##The probability vector of the test set\n",
    "    test_proba_svm_ = np.mean(np.delete(test_svm1_proba,0,2), axis=2)\n",
    "    test_proba_rf_ = np.mean(np.delete(test_rf1_proba,0,2), axis=2)\n",
    "    test_proba_lr_ = np.mean(np.delete(test_lr1_proba,0,2), axis=2)\n",
    "    test_proba_gbc_ = np.mean(np.delete(test_gbc1_proba,0,2), axis=2)\n",
    "\n",
    "##The statistical vector of the test set    \n",
    "test_p_svm_ = non_condition_p(cal_scores2_svm, test_svmnc2_score)\n",
    "test_p_rf_ = non_condition_p(cal_scores2_rf, test_rfnc2_score)\n",
    "test_p_lr_ = non_condition_p(cal_scores2_lr, test_lrnc2_score)   \n",
    "test_p_gbc_ = non_condition_p(cal_scores2_gbc, test_gbcnc2_score)    \n",
    "test_group_it = np.hstack((test_proba_svm_, test_proba_rf_, test_proba_lr_, test_proba_gbc_, \n",
    "                           test_p_svm_, test_p_rf_, test_p_lr_, test_p_gbc_))\n",
    "test_group_it = preprocessing.scale(test_group_it) \n",
    "\n",
    "\n",
    "##Determine whether the prediction of the original model is correct\n",
    "discarded_sample = [] \n",
    "accept_sample = [] \n",
    "discarded_right_sample = [] \n",
    "accept_right_sample = [] \n",
    "accept_or_reject = []   \n",
    "dis_test = np.empty((test_group_it.shape[0],class_num),float)\n",
    "for aa in range(class_num):\n",
    "    aa_prob = joblib.load('./save_model/clf_p_prob_%s'%aa+'.model')\n",
    "    dis_test[:,aa] = aa_prob.decision_function(test_group_it).flatten()            \n",
    "for t in range(len(y_pred_dif)):\n",
    "        result_it_d = np.argmax(dis_test[t,:])\n",
    "        svmit = joblib.load('./save_model/clf_pit.model')\n",
    "        result_it = svmit.predict(test_group_it[t].reshape(1,-1))\n",
    "        if (y_pred_dif[t] == result_it == result_it_d):\n",
    "            accept_sample.append(t)\n",
    "            accept_or_reject.append(1)\n",
    "            if(y_pred_dif[t] == y_true_dif[t]):\n",
    "                accept_right_sample.append(t)\n",
    "        else:\n",
    "            discarded_sample.append(t)\n",
    "            accept_or_reject.append(-1)\n",
    "            if(y_pred_dif[t] == y_true_dif[t]):\n",
    "                discarded_right_sample.append(t)   \n",
    "                \n",
    "print('\\n---------------  Evaluate the ability of the underlying model to predict the test data -----------------\\n') \n",
    "print(accept_or_reject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf692a9",
   "metadata": {},
   "source": [
    "## Step 4. The performance of the RISE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "205a1102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------ The performance of the RISE --------------------\n",
      "\n",
      "True Positive: 103\n",
      "False Positive: 7\n",
      "False Negative: 9\n",
      "True Negative: 398\n",
      "Accuracy: 0.9690522243713733\n",
      "Precision: 0.9363636363636364\n",
      "Recall: 0.9196428571428571\n",
      "F1_Score: 0.9279279279279279\n"
     ]
    }
   ],
   "source": [
    "reject_num = len(discarded_sample) \n",
    "reject_num_right =  len(discarded_right_sample) \n",
    "accept_num = len(accept_sample) \n",
    "accept_num_right = len(accept_right_sample) \n",
    "\n",
    "TP = reject_num - reject_num_right\n",
    "FP = reject_num_right\n",
    "FN = accept_num - accept_num_right\n",
    "TN = accept_num_right\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "Precision = TP/(TP+FP)\n",
    "Recall = TP/(TP+FN)\n",
    "F1_Score = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "print('\\n ------------------ The performance of the RISE --------------------\\n')\n",
    "print('True Positive:',TP)\n",
    "print('False Positive:',FP)\n",
    "print('False Negative:',FN)\n",
    "print('True Negative:', TN)\n",
    "print('Accuracy:',Accuracy)\n",
    "print('Precision:',Precision)\n",
    "print('Recall:',Recall)\n",
    "print('F1_Score:', F1_Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c90fb",
   "metadata": {},
   "source": [
    "## Step 5. The performance of the underlying model after RISE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b396d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------  The performance of the underlying model after RISE --------\n",
      "\n",
      "The accuracy with RISE:  0.9778869778869779\n",
      "Confusion matrix after RISE: \n",
      " [[  0   0   0   1   6]\n",
      " [  0 102   0   0   0]\n",
      " [  0   0 103   0   0]\n",
      " [  0   0   1 103   0]\n",
      " [  0   0   1   0  90]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print('\\n--------  The performance of the underlying model after RISE --------\\n')     \n",
    "if len(accept_sample) == 0:\n",
    "    print('null')\n",
    "else:\n",
    "    accept_sample_index = np.array(accept_sample)\n",
    "    accept_data = x_test1[accept_sample_index]\n",
    "    accept_label = y_test1[accept_sample_index]\n",
    "    clf_dif = myclassifier[class_index]\n",
    "    clf_dif.fit(x_train1,y_train1)\n",
    "    acc_aft = clf_dif.score(accept_data,accept_label)\n",
    "    print('The accuracy with RISE: ',acc_aft)\n",
    "    y_true_actrain, y_pred_actrain = accept_label,clf_dif.predict(accept_data)\n",
    "    aft_confusion_matrix = confusion_matrix(y_true_actrain, y_pred_actrain)\n",
    "    print('Confusion matrix after RISE: \\n',aft_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
