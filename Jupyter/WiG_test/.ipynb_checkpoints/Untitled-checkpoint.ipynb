{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e606f7",
   "metadata": {},
   "source": [
    "# Robust Wireless Sensing using Probabilistic and Statistical Assessments ：Artifact - Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd450db7",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "This interactive Jupyter notebook provides an example to show the performance of RISE. 主要包括：没有使用RISE时原模型的表现；根据训练数据为每个类训练一个Anomaly Detector；评估原模型对测试数据的预测能力；展示RISE的表现；展示原模型对RISE接受数据的识别能力。\n",
    "## Instructions for Experimental Workflow:\n",
    "Select each cell in turn and use “Cell” > “Run Cell” from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure all previous cells have been executed.\n",
    "## Important Notes\n",
    "#### Some cells can take a few minutes to complete; please wait for the results until step to the next cell.\n",
    "\n",
    "## Step 1. Performance of the underlying model without RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3544d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------test data is p1_ scenario-------------\n",
      "\n",
      "\n",
      "-------training data is ['p2_', 'p3_', 'm1_', 'm2_'] scenario----------\n",
      "\n",
      "\n",
      "--------  The performance of the underlying model without RISE --------\n",
      "\n",
      "The accuracy without RISE:  0.85\n",
      "Confusion matrix without RISE: \n",
      " [[30  0  0  0  0  0]\n",
      " [ 0 30  0  0  0  0]\n",
      " [ 6  0 24  0  0  0]\n",
      " [17  0  0 13  0  0]\n",
      " [ 0  0  0  0 30  0]\n",
      " [ 0  0  0  4  0 26]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn.ensemble\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nonconformist.nc import MarginErrFunc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Numerical issues were encountered \")\n",
    "import sys\n",
    "sys.path.insert(0,'../..')\n",
    "from Statistical_vector.statistical_vector import train_statistical_vector, test_statistical_vector_param, non_condition_p\n",
    "\n",
    "\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "myclassifier = [svm.SVC(probability = True, break_ties=True, decision_function_shape='ovr', random_state=0),\n",
    "                sklearn.ensemble.RandomForestClassifier(n_estimators=100,random_state=0),\n",
    "                DecisionTreeClassifier(random_state=0),neighbors.KNeighborsClassifier(n_neighbors=10),\n",
    "                LogisticRegression(random_state=0),GradientBoostingClassifier(n_estimators=100,random_state=0),\n",
    "                LinearDiscriminantAnalysis(), AdaBoostClassifier(),\n",
    "                GaussianNB(),QuadraticDiscriminantAnalysis()]   \n",
    "\n",
    "\n",
    "times = ['p1_'] ##test set\n",
    "train_name = ['p2_','p3_','m1_','m2_'] ##train set\n",
    "filepath = r'./data/' \n",
    "filename = ['wig9_label_startend30']\n",
    "class_index = 0 \n",
    "class_num = 6   \n",
    "\n",
    "\n",
    "##load test data\n",
    "print('\\n---------------test data is '  +  times[0] + ' scenario-------------\\n')\n",
    "data = sio.loadmat(filepath + filename[0] + times[0] + '.mat')\n",
    "xx2 = data['fe_wig']\n",
    "yy2 = data['label']\n",
    "yy2 = yy2.flatten()\n",
    "test_x = xx2\n",
    "test_y = yy2\n",
    "\n",
    "##load train data\n",
    "print('\\n-------training data is ' + str(train_name) + ' scenario----------\\n')\n",
    "xx1 = np.empty(shape=[0, xx2.shape[1]])\n",
    "yy1 = np.empty(shape=[1, 0],dtype=int)            \n",
    "for ii in train_name:\n",
    "    data = sio.loadmat(filepath + filename[0] + ii+ '.mat')\n",
    "    x1 = data['fe_wig']\n",
    "    y1 = data['label']\n",
    "    x1 = min_max_scaler.fit_transform(x1)\n",
    "    xx1 = np.append(xx1, x1, axis=0)\n",
    "    yy1 = np.append(yy1, y1, axis=1)\n",
    "yy1 = yy1.flatten()\n",
    "\n",
    "index = [t for t in range(xx1.shape[0])] \n",
    "random.shuffle(index)\n",
    "x_train11 = xx1[index]\n",
    "x_train1 = x_train11\n",
    "y_train1 = yy1[index]\n",
    "y_train1 = y_train1 - 1 \n",
    "   \n",
    "\n",
    "############################ Without RISE  ###############################\n",
    "print('\\n--------  The performance of the underlying model without RISE --------\\n')\n",
    "x_test1 = min_max_scaler.fit_transform(test_x)\n",
    "y_test1 = test_y\n",
    "y_test1 = y_test1 - 1\n",
    "clf_dif = myclassifier[class_index]\n",
    "clf_dif.fit(x_train1,y_train1)\n",
    "acc_dif = clf_dif.score(x_test1,y_test1)\n",
    "print('The accuracy without RISE: ',acc_dif)\n",
    "y_true_dif, y_pred_dif = y_test1,clf_dif.predict(x_test1)\n",
    "test_confusion_matrix = confusion_matrix(y_true_dif, y_pred_dif)\n",
    "print('Confusion matrix without RISE: \\n',test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c419d08",
   "metadata": {},
   "source": [
    "## Step 2. Training an Anomaly Detector for each class according to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbda47f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------  Training one-class SVM model for each class --------\n",
      "\n",
      "clf_p_prob_0\n",
      "\n",
      "clf_p_prob_1\n",
      "\n",
      "clf_p_prob_2\n",
      "\n",
      "clf_p_prob_3\n",
      "\n",
      "clf_p_prob_4\n",
      "\n",
      "clf_p_prob_5\n",
      "\n",
      "-------The number of classes 6 ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################ Trainning Anomaly Detector  ###############################  \n",
    "\n",
    "## Probability vector and statistical vector are calculated according to the training data\n",
    "skf = StratifiedKFold(n_splits=3, random_state=0,shuffle=True)\n",
    "cal_proba = np.empty(shape=[0, (class_num)*4])\n",
    "cal_score = np.empty(shape=[0, 1*4])\n",
    "train_proba = np.empty(shape=[0, (class_num)*4])\n",
    "train_nonconformity = np.empty(shape=[0, (class_num)*4])\n",
    "train_p = np.empty(shape=[0, (class_num)*4])\n",
    "cal_label = np.empty(shape=[0,1])\n",
    "train_label = np.empty(shape=[0,1])\n",
    "for train_index, cal_index in skf.split(x_train1, y_train1):\n",
    "    data_train = x_train1[train_index, :] \n",
    "    label_train = y_train1[train_index]\n",
    "    data_cal = x_train1[cal_index, :]\n",
    "    label_cal = y_train1[cal_index]\n",
    "    \n",
    "    train_p1_svm_, train_proba1_svm_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[0], non_Func = MarginErrFunc(), significance=None) \n",
    "    train_p1_rf_, train_proba1_rf_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[1], non_Func = MarginErrFunc(), significance=None) \n",
    "    train_p1_lr_, train_proba1_lr_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[4], non_Func = MarginErrFunc(), significance=None)\n",
    "    train_p1_gbc_, train_proba1_gbc_ = train_statistical_vector(data_train, label_train, data_cal, label_cal, \n",
    "                       classification_model = myclassifier[5], non_Func = MarginErrFunc(), significance=None)\n",
    "    \n",
    "    train_proba1 = np.hstack((train_proba1_svm_, train_proba1_rf_, train_proba1_lr_, train_proba1_gbc_))\n",
    "    train_p1 = np.hstack((train_p1_svm_, train_p1_rf_, train_p1_lr_, train_p1_gbc_))\n",
    "    train_proba = np.append(train_proba, train_proba1, axis=0)  \n",
    "    train_p = np.append(train_p, train_p1, axis=0)  \n",
    "    train_label = np.append(train_label,y_train1[train_index]) \n",
    "    \n",
    "p_thr = 0.1\n",
    "rows_ = []  \n",
    "for row_ in range(train_p.shape[0]):\n",
    "    if (max(train_p[row_,0:0+class_num]) > p_thr) & (max(train_p[row_,class_num:class_num*1+class_num]) > p_thr) \\\n",
    "        & (max(train_p[row_,class_num*2:class_num*2+class_num]) > p_thr) & (max(train_p[row_,class_num*3:class_num*3+class_num]) > p_thr) :               \n",
    "            rows_.append(row_)\n",
    "        \n",
    "pro_n_p = np.hstack((train_proba, train_p))\n",
    "group_it = pro_n_p[rows_,:] \n",
    "train_label1 = train_label[rows_] \n",
    "\n",
    "group_it = preprocessing.scale(group_it)\n",
    "   \n",
    "index = [t for t in range(group_it.shape[0])] \n",
    "random.shuffle(index)\n",
    "group_it1 = group_it[index] \n",
    "train_p_lable1 = train_label1[index]\n",
    "\n",
    "\n",
    "\n",
    "####Training an anomaly detector for each class\n",
    "\n",
    "##Generalize probability vectors and statistical vectors for each class\n",
    "names = locals()\n",
    "fe_p_prob = np.hstack((train_p_lable1.reshape(-1,1),group_it1))\n",
    "for tt in range(class_num):\n",
    "    names['fe_p_prob_%s'%tt] = fe_p_prob[fe_p_prob[:,0]==tt]\n",
    "    names['fe_p_prob_%s'%tt] = np.delete(names['fe_p_prob_%s'%tt], 0, 1)  \n",
    "\n",
    "print('\\n---------------  Training one-class SVM model for each class -----------------\\n')    \n",
    "##training one-class SVM model for each class\n",
    "for tt in range(class_num):\n",
    "    names['./save_model/clf_p_prob_%s'%tt] = svm.OneClassSVM(nu=0.5,kernel=\"linear\" )\n",
    "    names['./save_model/clf_p_prob_%s'%tt].fit(names['fe_p_prob_%s'%tt])\n",
    "    print('clf_p_prob_'+ str(tt)+'\\n')\n",
    "    ##save one-class SVM model for each class\n",
    "    joblib.dump(names['./save_model/clf_p_prob_%s'%tt],'./save_model/clf_p_prob_%s'%tt+'.model')\n",
    "    \n",
    "\n",
    "##Ensemble Learning\n",
    "clf_dif1 = sklearn.ensemble.RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "clf_dif2 = svm.SVC(probability = True, random_state=0)\n",
    "clf_dif3 = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "clf_dif4 = LogisticRegression(random_state=0)\n",
    "clf_dif7 = AdaBoostClassifier(random_state=0)   \n",
    "clf_pit = VotingClassifier(estimators = [('rf',clf_dif1),('svm',clf_dif2),\n",
    "('knn',clf_dif3),('lr',clf_dif4),('AdaBoost',clf_dif7)],voting='hard')      \n",
    "clf_pit.fit(group_it1,train_p_lable1)\n",
    "joblib.dump(clf_pit,'./save_model/clf_pit.model') \n",
    "\n",
    "print('---------------------- The number of classes is ' + str(class_num) + ' ----------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab9215",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the ability of the underlying model to predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b883e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------  Evaluate the ability of the underlying model to predict the test data -----------------\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#####Calculate the probability vector and statistical vector of the test set\n",
    "           \n",
    "calibration_portion = 0.5\n",
    "split = StratifiedShuffleSplit(n_splits=1,\n",
    "                   test_size=calibration_portion)\n",
    "for train, cal in split.split(x_train1,y_train1):\n",
    "    cal_scores1_svm = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_rf = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_lr = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    cal_scores1_gbc = np.empty(cal.reshape(-1,1).shape,dtype=float)\n",
    "    test_svmnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_rfnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_lrnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_gbcnc1_score = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_svm1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_rf1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_lr1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    test_gbc1_proba = np.empty(np.array([x_test1.shape[0],class_num]),dtype=float)\n",
    "    for repeat in range(10):\n",
    "        train_sample = np.random.choice(train.size, train.size, replace=True)\n",
    "        data_train = x_train1[train_sample, :]\n",
    "        label_train = y_train1[train_sample]\n",
    "        data_cal = x_train1[cal, :]\n",
    "        label_cal = y_train1[cal]\n",
    "        data_test = x_test1\n",
    "\n",
    "        cal_scores_svm, test_svmnc_score, test_svm_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[0], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_svm = np.hstack((cal_scores1_svm,cal_scores_svm.reshape(-1,1)))\n",
    "        test_svmnc1_score = np.dstack((test_svmnc1_score,test_svmnc_score))\n",
    "        test_svm1_proba = np.dstack((test_svm1_proba,test_svm_proba))\n",
    "  \n",
    "        cal_scores_rf, test_rfnc_score, test_rf_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[1], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_rf = np.hstack((cal_scores1_rf,cal_scores_rf.reshape(-1,1)))\n",
    "        test_rfnc1_score = np.dstack((test_rfnc1_score,test_rfnc_score))\n",
    "        test_rf1_proba = np.dstack((test_rf1_proba,test_rf_proba))\n",
    "          \n",
    "        cal_scores_lr, test_lrnc_score, test_lr_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[4], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_lr = np.hstack((cal_scores1_lr,cal_scores_lr.reshape(-1,1)))\n",
    "        test_lrnc1_score = np.dstack((test_lrnc1_score,test_lrnc_score))\n",
    "        test_lr1_proba = np.dstack((test_lr1_proba,test_lr_proba))\n",
    "        \n",
    "        cal_scores_gbc, test_gbcnc_score, test_gbc_proba = test_statistical_vector_param(data_train, label_train,  data_cal, label_cal, data_test,\n",
    "                 classification_model=myclassifier[5], non_Func = MarginErrFunc(), significance=None)\n",
    "        cal_scores1_gbc = np.hstack((cal_scores1_gbc,cal_scores_gbc.reshape(-1,1)))\n",
    "        test_gbcnc1_score = np.dstack((test_gbcnc1_score,test_gbcnc_score))\n",
    "        test_gbc1_proba = np.dstack((test_gbc1_proba,test_gbc_proba))\n",
    "        \n",
    "        \n",
    "    ##Nonconformity score of the validation set of the test set    \n",
    "    cal_scores2_svm = np.mean(np.delete(cal_scores1_svm,0,1), axis=1)\n",
    "    cal_scores2_rf = np.mean(np.delete(cal_scores1_rf,0,1), axis=1)\n",
    "    cal_scores2_lr = np.mean(np.delete(cal_scores1_lr,0,1), axis=1)\n",
    "    cal_scores2_gbc = np.mean(np.delete(cal_scores1_gbc,0,1), axis=1) \n",
    "    \n",
    "    ##Nonconformity score of the test set\n",
    "    test_svmnc2_score = np.mean(np.delete(test_svmnc1_score,0,2), axis=2)\n",
    "    test_rfnc2_score = np.mean(np.delete(test_rfnc1_score,0,2), axis=2)\n",
    "    test_lrnc2_score = np.mean(np.delete(test_lrnc1_score,0,2), axis=2)\n",
    "    test_gbcnc2_score = np.mean(np.delete(test_gbcnc1_score,0,2), axis=2)\n",
    "    \n",
    "    ##The probability vector of the test set\n",
    "    test_proba_svm_ = np.mean(np.delete(test_svm1_proba,0,2), axis=2)\n",
    "    test_proba_rf_ = np.mean(np.delete(test_rf1_proba,0,2), axis=2)\n",
    "    test_proba_lr_ = np.mean(np.delete(test_lr1_proba,0,2), axis=2)\n",
    "    test_proba_gbc_ = np.mean(np.delete(test_gbc1_proba,0,2), axis=2)\n",
    "\n",
    "##The statistical vector of the test set    \n",
    "test_p_svm_ = non_condition_p(cal_scores2_svm, test_svmnc2_score)\n",
    "test_p_rf_ = non_condition_p(cal_scores2_rf, test_rfnc2_score)\n",
    "test_p_lr_ = non_condition_p(cal_scores2_lr, test_lrnc2_score)   \n",
    "test_p_gbc_ = non_condition_p(cal_scores2_gbc, test_gbcnc2_score)    \n",
    "test_group_it = np.hstack((test_proba_svm_, test_proba_rf_, test_proba_lr_, test_proba_gbc_, \n",
    "                           test_p_svm_, test_p_rf_, test_p_lr_, test_p_gbc_))\n",
    "test_group_it = preprocessing.scale(test_group_it) \n",
    "\n",
    "\n",
    "##Determine whether the prediction of the original model is correct\n",
    "discarded_sample = [] \n",
    "accept_sample = [] \n",
    "discarded_right_sample = [] \n",
    "accept_right_sample = [] \n",
    "accept_or_reject = []   \n",
    "dis_test = np.empty((test_group_it.shape[0],class_num),float)\n",
    "for aa in range(class_num):\n",
    "    aa_prob = joblib.load('./save_model/clf_p_prob_%s'%aa+'.model')\n",
    "    dis_test[:,aa] = aa_prob.decision_function(test_group_it).flatten()            \n",
    "for t in range(len(y_pred_dif)):\n",
    "        result_it_d = np.argmax(dis_test[t,:])\n",
    "        svmit = joblib.load('./save_model/clf_pit.model')\n",
    "        result_it = svmit.predict(test_group_it[t].reshape(1,-1))\n",
    "        if (y_pred_dif[t] == result_it == result_it_d):\n",
    "            accept_sample.append(t)\n",
    "            accept_or_reject.append(1)\n",
    "            if(y_pred_dif[t] == y_true_dif[t]):\n",
    "                accept_right_sample.append(t)\n",
    "        else:\n",
    "            discarded_sample.append(t)\n",
    "            accept_or_reject.append(-1)\n",
    "            if(y_pred_dif[t] == y_true_dif[t]):\n",
    "                discarded_right_sample.append(t)   \n",
    "                \n",
    "print('\\n---------------  Evaluate the ability of the underlying model to predict the test data -----------------\\n') \n",
    "print(accept_or_reject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf692a9",
   "metadata": {},
   "source": [
    "## Step 4. The performance of the RISE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205a1102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------ The performance of the RISE --------------------\n",
      "\n",
      "True Positive: 26\n",
      "False Positive: 0\n",
      "False Negative: 1\n",
      "True Negative: 153\n",
      "Accuracy: 0.9944444444444445\n",
      "Precision: 1.0\n",
      "Recall: 0.9629629629629629\n",
      "F1_Score: 0.9811320754716981\n"
     ]
    }
   ],
   "source": [
    "reject_num = len(discarded_sample) \n",
    "reject_num_right =  len(discarded_right_sample) \n",
    "accept_num = len(accept_sample) \n",
    "accept_num_right = len(accept_right_sample) \n",
    "\n",
    "TP = reject_num - reject_num_right\n",
    "FP = reject_num_right\n",
    "FN = accept_num - accept_num_right\n",
    "TN = accept_num_right\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "Precision = TP/(TP+FP)\n",
    "Recall = TP/(TP+FN)\n",
    "F1_Score = 2 * Precision * Recall / (Precision + Recall)\n",
    "\n",
    "print('\\n ------------------ The performance of the RISE --------------------\\n')\n",
    "print('True Positive:',TP)\n",
    "print('False Positive:',FP)\n",
    "print('False Negative:',FN)\n",
    "print('True Negative:', TN)\n",
    "print('Accuracy:',Accuracy)\n",
    "print('Precision:',Precision)\n",
    "print('Recall:',Recall)\n",
    "print('F1_Score:', F1_Score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c90fb",
   "metadata": {},
   "source": [
    "## Step 5. The performance of the underlying model after RISE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b396d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------  The performance of the underlying model after RISE --------\n",
      "\n",
      "The accuracy with RISE:  0.9935064935064936\n",
      "Confusion matrix after RISE: \n",
      " [[30  0  0  0  0  0]\n",
      " [ 0 30  0  0  0  0]\n",
      " [ 1  0 24  0  0  0]\n",
      " [ 0  0  0 13  0  0]\n",
      " [ 0  0  0  0 30  0]\n",
      " [ 0  0  0  0  0 26]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n--------  The performance of the underlying model after RISE --------\\n')     \n",
    "if len(accept_sample) == 0:\n",
    "    print('null')\n",
    "else:\n",
    "    accept_sample_index = np.array(accept_sample)\n",
    "    accept_data = x_test1[accept_sample_index]\n",
    "    accept_label = y_test1[accept_sample_index]\n",
    "    clf_dif = myclassifier[class_index]\n",
    "    clf_dif.fit(x_train1,y_train1)\n",
    "    acc_aft = clf_dif.score(accept_data,accept_label)\n",
    "    print('The accuracy with RISE: ',acc_aft)\n",
    "    y_true_actrain, y_pred_actrain = accept_label,clf_dif.predict(accept_data)\n",
    "    aft_confusion_matrix = confusion_matrix(y_true_actrain, y_pred_actrain)\n",
    "    print('Confusion matrix after RISE: \\n',aft_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
